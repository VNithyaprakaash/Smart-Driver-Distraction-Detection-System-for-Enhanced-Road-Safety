{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WpH1EtVgd090"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh43Ber6EsH1"},"outputs":[],"source":["!pip install timm"]},{"cell_type":"markdown","metadata":{"id":"UbV2hbgTkJ56"},"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XicbYIVkIgS"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","\n","class BasicConv(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True,\n","                 bn=True, bias=False):\n","        super(BasicConv, self).__init__()\n","        self.out_channels = out_planes\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n","                              stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n","        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5,\n","                                 momentum=0.01, affine=True) if bn else None\n","        self.relu = nn.ReLU() if relu else None\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.bn is not None:\n","            x = self.bn(x)\n","        if self.relu is not None:\n","            x = self.relu(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"OZSNLVz7j6X_"},"source":["utils progressive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8z4yKGnmj1J0"},"outputs":[],"source":["import numpy as np\n","import torchvision\n","from torch.autograd import Variable\n","from torchvision import transforms\n","#from model import *\n","\n","def cosine_anneal_schedule(t, nb_epoch, lr):\n","    cos_inner = np.pi * (t % (nb_epoch))  # t - 1 is used when t has 1-based indexing.\n","    cos_inner /= (nb_epoch)\n","    cos_out = np.cos(cos_inner) + 1\n","\n","    return float(lr / 2 * cos_out)\n","\n","\n","\n","def model_info(model):  # Plots a line-by-line description of a PyTorch model\n","    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n","    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n","    print('\\n%5s %50s %9s %12s %20s %12s %12s' % ('layer', 'name', 'gradient', 'parameters', 'shape', 'mu', 'sigma'))\n","    for i, (name, p) in enumerate(model.named_parameters()):\n","        name = name.replace('module_list.', '')\n","        print('%5g %50s %9s %12g %20s %12.3g %12.3g' % (\n","            i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n","    print('Model Summary: %g layers, %g parameters, %g gradients\\n' % (i + 1, n_p, n_g))\n","\n","\n","def test(net, criterion, batch_size):\n","    net.eval()\n","    use_cuda = True\n","    test_loss = 0\n","    correct = 0\n","    correct_com = 0\n","    total = 0\n","    idx = 0\n","    device = torch.device(\"cuda\")\n","\n","    transform_test = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    testset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/test1',\n","                                               transform=transform_test)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        idx = batch_idx\n","        if use_cuda:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","        inputs, targets = Variable(inputs ), Variable(targets)\n","        output_1, output_2, output_3, output_concat= net(inputs)\n","        outputs_com = output_1 + output_2 + output_3 + output_concat\n","\n","        loss = criterion(output_concat, targets)\n","\n","        test_loss += loss.item()\n","        _, predicted = torch.max(output_concat.data, 1)\n","        _, predicted_com = torch.max(outputs_com.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","        correct_com += predicted_com.eq(targets.data).cpu().sum()\n","\n","        if batch_idx % 50 == 0:\n","            print('Step: %d | Loss: %.3f | Acc: %.3f%% (%d/%d) |Combined Acc: %.3f%% (%d/%d)' % (\n","            batch_idx, test_loss / (batch_idx + 1), 100. * float(correct) / total, correct, total, 100. * float(correct_com) / total, correct_com, total))\n","\n","    test_acc = 100. * float(correct) / total\n","    test_acc_en = 100. * float(correct_com) / total\n","    test_loss = test_loss / (idx + 1)\n","\n","    return test_acc, test_acc_en, test_loss"]},{"cell_type":"markdown","metadata":{"id":"sGreT09dlZ0o"},"source":["Train Teacher\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gr-Lfjj1jYfK"},"outputs":[],"source":["from __future__ import print_function\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","#from pytorchimagemodels\n","import timm\n","import os\n","import random\n","import imgaug.augmenters as iaa\n","class Features(nn.Module):\n","    def __init__(self, net_layers):\n","        super(Features, self).__init__()\n","        self.net_layer_0 = nn.Sequential(net_layers[0])\n","        self.net_layer_1 = nn.Sequential(net_layers[1])\n","        self.net_layer_2 = nn.Sequential(net_layers[2])\n","        self.net_layer_3 = nn.Sequential(net_layers[3])\n","        self.net_layer_4 = nn.Sequential(*net_layers[4])\n","        self.net_layer_5 = nn.Sequential(*net_layers[5])\n","        self.net_layer_6 = nn.Sequential(*net_layers[6])\n","        self.net_layer_7 = nn.Sequential(*net_layers[7])\n","    def forward(self, x):\n","        x = self.net_layer_0(x)\n","        x = self.net_layer_1(x)\n","        x = self.net_layer_2(x)\n","        x = self.net_layer_3(x)\n","        x = self.net_layer_4(x)\n","        x1 = self.net_layer_5(x)\n","        x2 = self.net_layer_6(x1)\n","        x3 = self.net_layer_7(x2)\n","        return x1, x2, x3\n","def img_progressive(x, limit, p=0.5):\n","    if random.random()<p:\n","        aug = iaa.MultiplyBrightness((1-limit, 1+limit))\n","\n","        x = x.permute(0, 2, 3, 1)\n","        x = x.cpu().numpy()\n","        x = (x*255).astype(np.uint8)\n","        x = aug(images=x)\n","        x = torch.from_numpy(x.astype(np.float32)).clone()\n","        x = x/255\n","        x = x.permute(0, 3, 1, 2)\n","    return x\n","class Network_Wrapper(nn.Module):\n","    def __init__(self, net_layers):\n","        super().__init__()\n","        self.Features = Features(net_layers)\n","        self.max_pool1 = nn.MaxPool2d(kernel_size=28, stride=1)\n","        self.max_pool2 = nn.MaxPool2d(kernel_size=14, stride=1)\n","        self.max_pool3 = nn.MaxPool2d(kernel_size=7, stride=1)\n","        self.conv_block1 = nn.Sequential(\n","            BasicConv(512, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier1 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10)\n","        )\n","        self.conv_block2 = nn.Sequential(\n","            BasicConv(1024, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier2 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","        self.conv_block3 = nn.Sequential(\n","            BasicConv(2048, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier3 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","        self.classifier_concat = nn.Sequential(\n","            nn.BatchNorm1d(1024 * 3),\n","            nn.Linear(1024 * 3, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x1, x2, x3 = self.Features(x)\n","        x1_ = self.conv_block1(x1)\n","        x1_ = self.max_pool1(x1_)\n","        x1_f = x1_.view(x1_.size(0), -1)\n","        x1_c = self.classifier1(x1_f)\n","        x2_ = self.conv_block2(x2)\n","        x2_ = self.max_pool2(x2_)\n","        x2_f = x2_.view(x2_.size(0), -1)\n","        x2_c = self.classifier2(x2_f)\n","        x3_ = self.conv_block3(x3)\n","        x3_ = self.max_pool3(x3_)\n","        x3_f = x3_.view(x3_.size(0), -1)\n","        x3_c = self.classifier3(x3_f)\n","\n","        x_c_all = torch.cat((x1_f, x2_f, x3_f), -1)\n","        x_c_all = self.classifier_concat(x_c_all)\n","\n","        return x1_c, x2_c, x3_c, x_c_all\n","\n","\n","def train(nb_epoch, batch_size, store_name, resume=False, start_epoch=0, model_path=None):\n","\n","    exp_dir = store_name\n","    try:\n","        os.stat(exp_dir)\n","    except:\n","        os.makedirs(exp_dir)\n","\n","    use_cuda = True\n","    print(use_cuda)\n","\n","    print('==> Preparing data..')\n","    transform_train = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.RandomCrop(224, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    trainset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/train',\n","                                                transform=transform_train)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    print(\"List of models\")\n","    for i in timm.list_models(pretrained=True):\n","        print(i)\n","    model_name = \"skresnext50_32x4d\"\n","    #model_name = \"resnet18\"\n","    print(model_name)\n","    #model_info(net)\n","    net = timm.create_model(model_name, pretrained=True, num_classes=10)\n","    model_info(net)\n","    net_layers = list(net.children())\n","    net_layers = net_layers[0:8]\n","    print\n","    net = Network_Wrapper(net_layers)\n","    model_info(net)\n","    print('Model %s created, param count: %d' %\n","          ('Created_model', sum([m.numel() for m in net.parameters()])))\n","\n","\n","    netp = torch.nn.DataParallel(net)\n","\n","    device = torch.device(\"cuda\")\n","    net.to(device)\n","    cudnn.benchmark = True\n","\n","    CELoss = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD([\n","        {'params': net.classifier_concat.parameters(), 'lr': 0.002},\n","        {'params': net.conv_block1.parameters(), 'lr': 0.002},\n","        {'params': net.classifier1.parameters(), 'lr': 0.002},\n","        {'params': net.conv_block2.parameters(), 'lr': 0.002},\n","        {'params': net.classifier2.parameters(), 'lr': 0.002},\n","        {'params': net.conv_block3.parameters(), 'lr': 0.002},\n","        {'params': net.classifier3.parameters(), 'lr': 0.002},\n","        {'params': net.Features.parameters(), 'lr': 0.0002}\n","    ],\n","        momentum=0.9, weight_decay=5e-4)\n","\n","\n","    max_val_acc = 0\n","    lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.0002, 0.0002, 0.0002, 0.0002]\n","    for epoch in range(start_epoch, nb_epoch):\n","        print('\\nEpoch: %d' % epoch)\n","        net.train()\n","        train_loss = 0\n","        train_loss1 = 0\n","        train_loss2 = 0\n","        train_loss3 = 0\n","        train_loss4 = 0\n","        correct = 0\n","        total = 0\n","        idx = 0\n","        NORM = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","\n","        for batch_idx, (inputs, targets) in enumerate(trainloader):\n","            inputs1 = img_progressive(inputs.clone(), 0.3, p=0.3)\n","            inputs2 = img_progressive(inputs.clone(), 0.2, p=0.3)\n","            inputs3 = img_progressive(inputs.clone(), 0.1, p=0.3)\n","\n","            inputs = NORM(inputs)\n","            inputs1 = NORM(inputs1)\n","            inputs2 = NORM(inputs2)\n","            inputs3 = NORM(inputs3)\n","\n","\n","            idx = batch_idx\n","            if inputs.shape[0] < batch_size:\n","                continue\n","            if use_cuda:\n","                inputs, targets, inputs1, inputs2, inputs3 = inputs.to(device), targets.to(device), inputs1.to(device), inputs2.to(device), inputs3.to(device)\n","\n","            inputs, targets , inputs1, inputs2, inputs3 = Variable(inputs), Variable(targets), Variable(inputs1), Variable(inputs2), Variable(inputs3)\n","\n","            # update learning rate\n","            for nlr in range(len(optimizer.param_groups)):\n","                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, nb_epoch, lr[nlr])\n","\n","\n","            optimizer.zero_grad()\n","            output_1, _, _, _ = netp(inputs1)\n","            loss1 = CELoss(output_1, targets) * 1\n","            loss1.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()\n","            _, output_2, _, _ = netp(inputs2)\n","            loss2 = CELoss(output_2, targets) * 1\n","            loss2.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()\n","            _, _, output_3, _ = netp(inputs3)\n","            loss3 = CELoss(output_3, targets) * 1\n","            loss3.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()\n","            _, _, _, output_concat = netp(inputs)\n","            concat_loss = CELoss(output_concat, targets) * 2\n","            concat_loss.backward()\n","            optimizer.step()\n","\n","            #  training log\n","            _, predicted = torch.max(output_concat.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","\n","            train_loss += (loss1.item() + loss2.item() + loss3.item() + concat_loss.item())\n","            train_loss1 += loss1.item()\n","            train_loss2 += loss2.item()\n","            train_loss3 += loss3.item()\n","            train_loss4 += concat_loss.item()\n","\n","            if batch_idx % 50 == 0:\n","                print(\n","                    'Step: %d | Loss1: %.3f | Loss2: %.5f | Loss3: %.5f | Loss_concat: %.5f | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (\n","                    batch_idx, train_loss1 / (batch_idx + 1), train_loss2 / (batch_idx + 1),\n","                    train_loss3 / (batch_idx + 1), train_loss4 / (batch_idx + 1), train_loss / (batch_idx + 1),\n","                    100. * float(correct) / total, correct, total))\n","\n","        train_acc = 100. * float(correct) / total\n","        train_loss = train_loss / (idx + 1)\n","        with open(exp_dir + '/results_train.txt', 'a') as file:\n","            file.write(\n","                'Iteration %d | train_acc = %.5f | train_loss = %.5f | Loss1: %.3f | Loss2: %.5f | Loss3: %.5f | Loss_concat: %.5f |\\n' % (\n","                epoch, train_acc, train_loss, train_loss1 / (idx + 1), train_loss2 / (idx + 1), train_loss3 / (idx + 1),\n","                train_loss4 / (idx + 1)))\n","\n","        if epoch < 10 :#or epoch >= 50:\n","            val_acc, val_acc_com, val_loss = test(net, CELoss, 8)\n","            if val_acc_com > max_val_acc:\n","                max_val_acc = val_acc_com\n","                net.cpu()\n","                torch.save(net,  store_name + '/model%d.pth'%(max_val_acc))\n","                net.to(device)\n","            with open(exp_dir + '/results_test.txt', 'a') as file:\n","                file.write('Iteration %d, test_acc = %.5f, test_acc_combined = %.5f, test_loss = %.6f\\n' % (\n","                epoch, val_acc, val_acc_com, val_loss))\n","        else:\n","             net.cpu()\n","             torch.save(net,  store_name + '/model.pth')\n","\n","             net.to(device)\n","        torch.cuda.memory_summary()\n","\n","\n","if __name__ == '__main__':\n","    save_path = '/content/gdrive/MyDrive/STF/save_teacher'\n","    if not os.path.exists(save_path):\n","        os.mkdir(save_path)\n","    train(nb_epoch=10,  # number of epoch\n","          batch_size=32,  # batch size\n","          store_name=save_path,  # folder for output\n","          resume=False,  # resume training from checkpoint\n","          start_epoch=0,  # the start epoch number when you resume the training\n","          model_path='')  # the saved model where you want to resume the training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBFtezV2dsVz"},"outputs":[],"source":["from __future__ import print_function\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","#from pytorchimagemodels\n","import timm\n","import os\n","import random\n","import imgaug.augmenters as iaa\n","\n","class Features(nn.Module):\n","    def __init__(self, net_layers):\n","        super(Features, self).__init__()\n","        self.net_layer_0 = nn.Sequential(net_layers[0])\n","        self.net_layer_1 = nn.Sequential(net_layers[1])\n","        self.net_layer_2 = nn.Sequential(net_layers[2])\n","        self.net_layer_3 = nn.Sequential(net_layers[3])\n","        self.net_layer_4 = nn.Sequential(*net_layers[4])\n","        self.net_layer_5 = nn.Sequential(*net_layers[5])\n","        self.net_layer_6 = nn.Sequential(*net_layers[6])\n","        self.net_layer_7 = nn.Sequential(*net_layers[7])\n","\n","\n","    def forward(self, x):\n","        x = self.net_layer_0(x)\n","        x = self.net_layer_1(x)\n","        x = self.net_layer_2(x)\n","        x = self.net_layer_3(x)\n","        x = self.net_layer_4(x)\n","        x1 = self.net_layer_5(x)\n","        x2 = self.net_layer_6(x1)\n","        x3 = self.net_layer_7(x2)\n","        return x1, x2, x3\n","\n","\n","\n","def img_progressive(x, limit, p=0.5):\n","    if random.random()<p:\n","        aug = iaa.MultiplyBrightness((1-limit, 1+limit))\n","\n","        x = x.permute(0, 2, 3, 1)\n","        x = x.cpu().numpy()\n","        x = (x*255).astype(np.uint8)\n","        x = aug(images=x)\n","        x = torch.from_numpy(x.astype(np.float32)).clone()\n","        x = x/255\n","        x = x.permute(0, 3, 1, 2)\n","    return x\n","\n","\n","\n","class Network_Wrapper(nn.Module):\n","    def __init__(self, net_layers):\n","        super().__init__()\n","        self.Features = Features(net_layers)\n","\n","        self.max_pool1 = nn.MaxPool2d(kernel_size=28, stride=1)\n","        self.max_pool2 = nn.MaxPool2d(kernel_size=14, stride=1)\n","        self.max_pool3 = nn.MaxPool2d(kernel_size=7, stride=1)\n","\n","        self.conv_block1 = nn.Sequential(\n","            BasicConv(512, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier1 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10)\n","        )\n","\n","        self.conv_block2 = nn.Sequential(\n","            BasicConv(1024, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier2 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","        self.conv_block3 = nn.Sequential(\n","            BasicConv(2048, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier3 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","        self.classifier_concat = nn.Sequential(\n","            nn.BatchNorm1d(1024 * 3),\n","            nn.Linear(1024 * 3, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x1, x2, x3 = self.Features(x)\n","\n","        x1_ = self.conv_block1(x1)\n","        x1_ = self.max_pool1(x1_)\n","        x1_f = x1_.view(x1_.size(0), -1)\n","\n","        x1_c = self.classifier1(x1_f)\n","\n","        x2_ = self.conv_block2(x2)\n","        x2_ = self.max_pool2(x2_)\n","        x2_f = x2_.view(x2_.size(0), -1)\n","        x2_c = self.classifier2(x2_f)\n","\n","        x3_ = self.conv_block3(x3)\n","        x3_ = self.max_pool3(x3_)\n","        x3_f = x3_.view(x3_.size(0), -1)\n","        x3_c = self.classifier3(x3_f)\n","\n","        x_c_all = torch.cat((x1_f, x2_f, x3_f), -1)\n","        x_c_all = self.classifier_concat(x_c_all)\n","\n","        return x1_c, x2_c, x3_c, x_c_all\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFOFXiILoT-h"},"outputs":[],"source":["def train(nb_epoch, batch_size, store_name, resume=False, start_epoch=0, model_path=None):\n","\n","    exp_dir = store_name\n","    try:\n","        os.stat(exp_dir)\n","    except:\n","        os.makedirs(exp_dir)\n","\n","    use_cuda = True\n","    print(use_cuda)\n","\n","    print('==> Preparing data..')\n","    transform_train = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.RandomCrop(224, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    trainset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/train',\n","                                                transform=transform_train)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    print(\"List of models\")\n","    for i in timm.list_models(pretrained=True):\n","        print(i)\n","    #model_name = \"skresnext50_32x4d\"\n","    model_name = \"resnet18\"\n","    net = timm.create_model(model_name, pretrained=True, num_classes=10)\n","\n","    net_layers = list(net.children())\n","    net_layers = net_layers[0:8]\n","\n","    net = Network_Wrapper(net_layers)\n","    model_info(net)\n","    print('Model %s created, param count: %d' %\n","          ('Created_model', sum([m.numel() for m in net.parameters()])))\n","\n","\n","    netp = torch.nn.DataParallel(net)\n","\n","    device = torch.device(\"cuda\")\n","    net.to(device)\n","    cudnn.benchmark = True\n","\n","    CELoss = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD([\n","        {'params': net.classifier_concat.parameters(), 'lr': 0.002},\n","        {'params': net.conv_block1.parameters(), 'lr': 0.002},\n","        {'params': net.classifier1.parameters(), 'lr': 0.002},\n","        {'params': net.conv_block2.parameters(), 'lr': 0.002},\n","        {'params': net.classifier2.parameters(), 'lr': 0.002},\n","        {'params': net.conv_block3.parameters(), 'lr': 0.002},\n","        {'params': net.classifier3.parameters(), 'lr': 0.002},\n","        {'params': net.Features.parameters(), 'lr': 0.0002}\n","    ],\n","        momentum=0.9, weight_decay=5e-4)\n","\n","\n","    max_val_acc = 0\n","    lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.0002, 0.0002, 0.0002, 0.0002]\n","    for epoch in range(start_epoch, nb_epoch):\n","        print('\\nEpoch: %d' % epoch)\n","        net.train()\n","        train_loss = 0\n","        train_loss1 = 0\n","        train_loss2 = 0\n","        train_loss3 = 0\n","        train_loss4 = 0\n","        correct = 0\n","        total = 0\n","        idx = 0\n","        NORM = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","\n","        for batch_idx, (inputs, targets) in enumerate(trainloader):\n","            inputs1 = img_progressive(inputs.clone(), 0.3, p=0.3)\n","            inputs2 = img_progressive(inputs.clone(), 0.2, p=0.3)\n","            inputs3 = img_progressive(inputs.clone(), 0.1, p=0.3)\n","\n","            inputs = NORM(inputs)\n","            inputs1 = NORM(inputs1)\n","            inputs2 = NORM(inputs2)\n","            inputs3 = NORM(inputs3)\n","\n","\n","            idx = batch_idx\n","            if inputs.shape[0] < batch_size:\n","                continue\n","            if use_cuda:\n","                inputs, targets, inputs1, inputs2, inputs3 = inputs.to(device), targets.to(device), inputs1.to(device), inputs2.to(device), inputs3.to(device)\n","\n","            inputs, targets , inputs1, inputs2, inputs3 = Variable(inputs), Variable(targets), Variable(inputs1), Variable(inputs2), Variable(inputs3)\n","\n","            # update learning rate\n","            for nlr in range(len(optimizer.param_groups)):\n","                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, nb_epoch, lr[nlr])\n","\n","\n","            optimizer.zero_grad()\n","            output_1, _, _, _ = netp(inputs1)\n","            loss1 = CELoss(output_1, targets) * 1\n","            loss1.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()\n","            _, output_2, _, _ = netp(inputs2)\n","            loss2 = CELoss(output_2, targets) * 1\n","            loss2.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()\n","            _, _, output_3, _ = netp(inputs3)\n","            loss3 = CELoss(output_3, targets) * 1\n","            loss3.backward()\n","            optimizer.step()\n","\n","            optimizer.zero_grad()\n","            _, _, _, output_concat = netp(inputs)\n","            concat_loss = CELoss(output_concat, targets) * 2\n","            concat_loss.backward()\n","            optimizer.step()\n","\n","            #  training log\n","            _, predicted = torch.max(output_concat.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","\n","            train_loss += (loss1.item() + loss2.item() + loss3.item() + concat_loss.item())\n","            train_loss1 += loss1.item()\n","            train_loss2 += loss2.item()\n","            train_loss3 += loss3.item()\n","            train_loss4 += concat_loss.item()\n","\n","            if batch_idx % 50 == 0:\n","                print(\n","                    'Step: %d | Loss1: %.3f | Loss2: %.5f | Loss3: %.5f | Loss_concat: %.5f | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (\n","                    batch_idx, train_loss1 / (batch_idx + 1), train_loss2 / (batch_idx + 1),\n","                    train_loss3 / (batch_idx + 1), train_loss4 / (batch_idx + 1), train_loss / (batch_idx + 1),\n","                    100. * float(correct) / total, correct, total))\n","\n","        train_acc = 100. * float(correct) / total\n","        train_loss = train_loss / (idx + 1)\n","        with open(exp_dir + '/results_train.txt', 'a') as file:\n","            file.write(\n","                'Iteration %d | train_acc = %.5f | train_loss = %.5f | Loss1: %.3f | Loss2: %.5f | Loss3: %.5f | Loss_concat: %.5f |\\n' % (\n","                epoch, train_acc, train_loss, train_loss1 / (idx + 1), train_loss2 / (idx + 1), train_loss3 / (idx + 1),\n","                train_loss4 / (idx + 1)))\n","\n","        if epoch < 10 :#or epoch >= 50:\n","            val_acc, val_acc_com, val_loss = test(net, CELoss, 8)\n","            if val_acc_com > max_val_acc:\n","                max_val_acc = val_acc_com\n","                net.cpu()\n","                torch.save(net,  store_name + '/model%d.pth'%(max_val_acc))\n","                net.to(device)\n","            with open(exp_dir + '/results_test.txt', 'a') as file:\n","                file.write('Iteration %d, test_acc = %.5f, test_acc_combined = %.5f, test_loss = %.6f\\n' % (\n","                epoch, val_acc, val_acc_com, val_loss))\n","        else:\n","             net.cpu()\n","             torch.save(net,  store_name + '/model.pth')\n","\n","             net.to(device)\n","        torch.cuda.memory_summary()\n","\n","\n","if __name__ == '__main__':\n","    save_path = '/content/gdrive/MyDrive/STF/save_teacher'\n","    if not os.path.exists(save_path):\n","        os.mkdir(save_path)\n","    train(nb_epoch=10,  # number of epoch\n","          batch_size=32,  # batch size\n","          store_name=save_path,  # folder for output\n","          resume=False,  # resume training from checkpoint\n","          start_epoch=0,  # the start epoch number when you resume the training\n","          model_path='')  # the saved model where you want to resume the training"]},{"cell_type":"markdown","metadata":{"id":"9dYov-m31ETV"},"source":["pyconv models"]},{"cell_type":"markdown","metadata":{"id":"C5d8x5Xaa00f"},"source":["PYCONV2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cenhpFRya2fH"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, groups=1):\n","    \"\"\"standard convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n","                     padding=padding, dilation=dilation, groups=groups, bias=False)\n","\n","class PyConv2(nn.Module):\n","\n","    def __init__(self, inplans, planes,pyconv_kernels=[3, 5], stride=1, pyconv_groups=[1, 4]):\n","        super(PyConv2, self).__init__()\n","        self.conv2_1 = conv(inplans, planes // 2, kernel_size=pyconv_kernels[0], padding=pyconv_kernels[0] // 2,\n","                            stride=stride, groups=pyconv_groups[0])\n","        self.conv2_2 = conv(inplans, planes // 2, kernel_size=pyconv_kernels[1], padding=pyconv_kernels[1] // 2,\n","                            stride=stride, groups=pyconv_groups[1])\n","\n","    def forward(self, x):\n","        return torch.cat((self.conv2_1(x), self.conv2_2(x)), dim=1)\n"]},{"cell_type":"markdown","metadata":{"id":"nRwWXiWu2YIV"},"source":["logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"op_6w0Pj2ZWa"},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import print_function\n","from __future__ import division\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Logits(nn.Module):\n","\tdef __init__(self):\n","\t\tsuper(Logits, self).__init__()\n","\n","\tdef forward(self, out_s, out_t):\n","\t\tloss = F.mse_loss(out_s, out_t)\n","\n","\t\treturn loss\n"]},{"cell_type":"markdown","metadata":{"id":"ahAMTTQoO_08"},"source":["traindriversearch"]},{"cell_type":"markdown","metadata":{"id":"H_h90df9Sn91"},"source":["utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBNVf-gFSnV4"},"outputs":[],"source":["import numpy as np\n","import torchvision\n","from torch.autograd import Variable\n","from torchvision import transforms\n","#from model import *\n","\n","\n","\n","def cosine_anneal_schedule(t, nb_epoch, lr):\n","    cos_inner = np.pi * (t % (nb_epoch))  # t - 1 is used when t has 1-based indexing.\n","    cos_inner /= (nb_epoch)\n","    cos_out = np.cos(cos_inner) + 1\n","\n","    return float(lr / 2 * cos_out)\n","\n","\n","\n","def model_info(model):  # Plots a line-by-line description of a PyTorch model\n","    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n","    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n","    print('\\n%5s %50s %9s %12s %20s %12s %12s' % ('layer', 'name', 'gradient', 'parameters', 'shape', 'mu', 'sigma'))\n","    for i, (name, p) in enumerate(model.named_parameters()):\n","        name = name.replace('module_list.', '')\n","        print('%5g %50s %9s %12g %20s %12.3g %12.3g' % (\n","            i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n","    print('Model Summary: %g layers, %g parameters, %g gradients\\n' % (i + 1, n_p, n_g))\n","\n","\n","def test_search(net, criterion, batch_size):\n","    net.eval()\n","    use_cuda = torch.cuda.is_available()\n","    test_loss = 0\n","    correct = 0\n","    correct_com = 0\n","    total = 0\n","    idx = 0\n","    device = torch.device(\"cuda\")\n","\n","    transform_test = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    testset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/test1',\n","                                               transform=transform_test)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            idx = batch_idx\n","            if use_cuda:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","            inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n","            output = net(inputs)\n","\n","            loss = criterion(output, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = torch.max(output.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","\n","            if batch_idx % 50 == 0 or batch_idx == testloader.__len__()-1:\n","                print('Step: %d | Loss: %.3f | Acc: %.3f%% (%d/%d) |' % (\n","                batch_idx, test_loss / (batch_idx + 1), 100. * float(correct) / total, correct, total))\n","\n","    test_acc = 100. * float(correct) / total\n","    test_loss = test_loss / (idx + 1)\n","\n","    return test_acc, test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsmq6afvPFxl"},"outputs":[],"source":["from __future__ import print_function\n","import os\n","import logging\n","import torch.optim as optim\n","\n","\n","class BasicLayer(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, kernel_size_, group=1, group_=4):\n","        super(BasicLayer, self).__init__()\n","        self.convs = nn.Sequential(\n","            PyConv2(in_planes, out_planes, pyconv_kernels=[kernel_size, kernel_size_], stride=1, pyconv_groups=[group, group_]),\n","            nn.BatchNorm2d(out_planes),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        return x\n","\n","\n","class Cells1(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(4, 1))\n","\n","        self.cell1 = BasicLayer(3, 32, kernel_size=11, kernel_size_=7, group=1, group_=1)\n","\n","        self.cell2 = BasicLayer(3, 32, kernel_size=11, kernel_size_=5, group=1, group_=1)\n","\n","        self.cell3 = BasicLayer(3, 32, kernel_size=11, kernel_size_=3, group=1, group_=1)\n","\n","        self.cell4 = BasicLayer(3, 32, kernel_size=11, kernel_size_=1, group=1, group_=1)\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        x3 = self.cell3(x)\n","        x4 = self.cell4(x)\n","\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","\n","        x = x1*p[0] + x2*p[1] + x3*p[2] + x4*p[3]\n","        return x\n","\n","\n","class Cells1_pool(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(2, 1))\n","\n","        self.cell1 = nn.AvgPool2d(2, stride=2)\n","\n","        self.cell2 = nn.MaxPool2d(2, stride=2)\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","        x = x1*p[0] + x2*p[1]\n","        return x\n","\n","\n","class Cells2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(9, 1))\n","\n","        self.cell1 = BasicLayer(32, 64, kernel_size=9, kernel_size_=5, group=1, group_=1)\n","        self.cell2 = BasicLayer(32, 64, kernel_size=9, kernel_size_=5, group=1, group_=2)\n","        self.cell3 = BasicLayer(32, 64, kernel_size=9, kernel_size_=5)\n","\n","        self.cell4 = BasicLayer(32, 64, kernel_size=9, kernel_size_=3, group=1, group_=1)\n","        self.cell5 = BasicLayer(32, 64, kernel_size=9, kernel_size_=3, group=1, group_=2)\n","        self.cell6 = BasicLayer(32, 64, kernel_size=9, kernel_size_=3)\n","\n","        self.cell7 = BasicLayer(32, 64, kernel_size=9, kernel_size_=1, group=1, group_=1)\n","        self.cell8 = BasicLayer(32, 64, kernel_size=9, kernel_size_=1, group=1, group_=2)\n","        self.cell9 = BasicLayer(32, 64, kernel_size=9, kernel_size_=1)\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        x3 = self.cell3(x)\n","        x4 = self.cell4(x)\n","        x5 = self.cell5(x)\n","        x6 = self.cell6(x)\n","        x7 = self.cell7(x)\n","        x8 = self.cell8(x)\n","        x9 = self.cell9(x)\n","\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","\n","        x = x1*p[0] + x2*p[1] + x3*p[2] + x4*p[3] + x5*p[4] + x6*p[5] + x7*p[6] + x8*p[7] + x9*p[8]\n","        return x\n","\n","class Cells2_pool(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(2, 1))\n","\n","        self.cell1 = nn.AvgPool2d(2, stride=2)\n","\n","        self.cell2 = nn.MaxPool2d(2, stride=2)\n","\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","        x = x1*p[0] + x2*p[1]\n","        return x\n","\n","\n","class Cells3(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(6, 1))\n","\n","        self.cell1 = BasicLayer(64, 128, kernel_size=5, kernel_size_=3, group=1, group_=1)\n","        self.cell2 = BasicLayer(64, 128, kernel_size=5, kernel_size_=3, group=1, group_=2)\n","        self.cell3 = BasicLayer(64, 128, kernel_size=5, kernel_size_=3)\n","\n","        self.cell4 = BasicLayer(64, 128, kernel_size=5, kernel_size_=1, group=1, group_=1)\n","        self.cell5 = BasicLayer(64, 128, kernel_size=5, kernel_size_=1, group=1, group_=2)\n","        self.cell6 = BasicLayer(64, 128, kernel_size=5, kernel_size_=1)\n","\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        x3 = self.cell3(x)\n","        x4 = self.cell4(x)\n","        x5 = self.cell5(x)\n","        x6 = self.cell6(x)\n","\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","\n","        x = x1*p[0] + x2*p[1] + x3*p[2] + x4*p[3] + x5*p[4] + x6*p[5]\n","        return x\n","\n","class Cells3_pool(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(2, 1))\n","\n","        self.cell1 = nn.AvgPool2d(2, stride=2)\n","\n","        self.cell2 = nn.MaxPool2d(2, stride=2)\n","\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","        x = x1*p[0] + x2*p[1]\n","        return x\n","\n","\n","\n","class Cells4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(3, 1))\n","\n","        self.cell1 = BasicLayer(128, 256, kernel_size=3, kernel_size_=1, group=1, group_=1)\n","        self.cell2 = BasicLayer(128, 256, kernel_size=3, kernel_size_=1, group=1, group_=2)\n","        self.cell3 = BasicLayer(128, 256, kernel_size=3, kernel_size_=1)\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        x3 = self.cell3(x)\n","\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","\n","        x = x1*p[0] + x2*p[1] + x3*p[2]\n","        return x\n","\n","class Cells4_pool(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.params = nn.Parameter(torch.ones(3, 1))\n","\n","        self.cell1 = nn.AvgPool2d(2, stride=2)\n","\n","        self.cell2 = nn.MaxPool2d(2, stride=2)\n","\n","    def forward(self, x):\n","        x1 = self.cell1(x)\n","        x2 = self.cell2(x)\n","        p = torch.nn.functional.softmax(self.params, dim=0)\n","        x = x1*p[0] + x2*p[1]\n","        return x\n","\n","\n","class Network_Wrapper(nn.Module):\n","    def __init__(self, net_layers):\n","        super().__init__()\n","        self.Features = Features(net_layers)\n","\n","        self.max_pool1 = nn.MaxPool2d(kernel_size=28, stride=1)\n","        self.max_pool2 = nn.MaxPool2d(kernel_size=14, stride=1)\n","        self.max_pool3 = nn.MaxPool2d(kernel_size=7, stride=1)\n","\n","        self.conv_block1 = nn.Sequential(\n","            BasicConv(512, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier1 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10)\n","        )\n","\n","        self.conv_block2 = nn.Sequential(\n","            BasicConv(1024, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier2 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","        self.conv_block3 = nn.Sequential(\n","            BasicConv(2048, 512, kernel_size=1, stride=1, padding=0, relu=True),\n","            BasicConv(512, 1024, kernel_size=3, stride=1, padding=1, relu=True)\n","        )\n","        self.classifier3 = nn.Sequential(\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","        self.classifier_concat = nn.Sequential(\n","            nn.BatchNorm1d(1024 * 3),\n","            nn.Linear(1024 * 3, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ELU(inplace=True),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x1, x2, x3 = self.Features(x)\n","\n","        x1_ = self.conv_block1(x1)\n","        x1_ = self.max_pool1(x1_)\n","        x1_f = x1_.view(x1_.size(0), -1)\n","\n","        x1_c = self.classifier1(x1_f)\n","\n","\n","        x2_ = self.conv_block2(x2)\n","        x2_ = self.max_pool2(x2_)\n","        x2_f = x2_.view(x2_.size(0), -1)\n","        x2_c = self.classifier2(x2_f)\n","\n","        x3_ = self.conv_block3(x3)\n","        x3_ = self.max_pool3(x3_)\n","        x3_f = x3_.view(x3_.size(0), -1)\n","        x3_c = self.classifier3(x3_f)\n","\n","        x_c_all = torch.cat((x1_f, x2_f, x3_f), -1)\n","        x_c_all = self.classifier_concat(x_c_all)\n","\n","        return x1_c, x2_c, x3_c, x_c_all\n","\n","\n","class Features(nn.Module):\n","    def __init__(self, net_layers):\n","        super(Features, self).__init__()\n","        self.net_layer_0 = nn.Sequential(net_layers[0])\n","        self.net_layer_1 = nn.Sequential(net_layers[1])\n","        self.net_layer_2 = nn.Sequential(net_layers[2])\n","        self.net_layer_3 = nn.Sequential(net_layers[3])\n","        self.net_layer_4 = nn.Sequential(*net_layers[4])\n","        self.net_layer_5 = nn.Sequential(*net_layers[5])\n","        self.net_layer_6 = nn.Sequential(*net_layers[6])\n","        self.net_layer_7 = nn.Sequential(*net_layers[7])\n","\n","\n","    def forward(self, x):\n","        x = self.net_layer_0(x)\n","        x = self.net_layer_1(x)\n","        x = self.net_layer_2(x)\n","        x = self.net_layer_3(x)\n","        x = self.net_layer_4(x)\n","        x1 = self.net_layer_5(x)\n","        x2 = self.net_layer_6(x1)\n","        x3 = self.net_layer_7(x2)\n","        return x1, x2, x3\n","\n","\n","class Search_Wrapper(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.L1 = Cells1()\n","        self.L1P = Cells1_pool()\n","        self.L2 = Cells2()\n","        self.L2P = Cells2_pool()\n","        self.L3 = Cells3()\n","        self.L3P = Cells3_pool()\n","        self.L4 = Cells4()\n","        self.L4P = Cells4_pool()\n","        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(256, 10)\n","        )\n","\n","    def forward(self, x):\n","        x = self.L1(x)\n","        x = self.L1P(x)\n","        x = self.L2(x)\n","        x = self.L2P(x)\n","        x = self.L3(x)\n","        x = self.L3P(x)\n","        x = self.L4(x)\n","        x = self.L4P(x)\n","        x = self.pool(x).view(x.shape[0], -1)\n","        x = self.classifier(x)\n","        return x\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdIiEDVVoMny"},"outputs":[],"source":["def train(nb_epoch, batch_size, store_name, resume=False, start_epoch=0, model_path=None):\n","    criterionKD = Logits()\n","    _logger = logging.getLogger('train')\n","\n","    exp_dir = store_name\n","    try:\n","        os.stat(exp_dir)\n","    except:\n","        os.makedirs(exp_dir)\n","\n","    use_cuda = True\n","    print(use_cuda)\n","\n","\n","    print('==> Preparing data..')\n","    transform_train = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.RandomCrop(224, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    trainset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/train/',\n","                                                transform=transform_train)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n","    net_teacher = torch.load(\"/content/gdrive/MyDrive/STF/save_teacher/model99.510.pth\")\n","\n","    net = Search_Wrapper()\n","\n","\n","    model_info(net)\n","    print('Model %s created, param count: %d' %\n","          ('Created_model', sum([m.numel() for m in net.parameters()])))\n","    net = torch.nn.DataParallel(net).cuda()\n","    net_teacher = torch.nn.DataParallel(net_teacher).cuda()\n","    print('Model %s created, param count: %d' %\n","          ('Created_model', sum([m.numel() for m in net_teacher.parameters()])))\n","\n","    device = torch.device(\"cuda\")\n","    net.to(device)\n","\n","\n","    CELoss = nn.CrossEntropyLoss()\n","\n","\n","    optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9, weight_decay=5e-4)\n","\n","    max_val_acc = 0\n","    lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n","    for epoch in range(start_epoch, nb_epoch):\n","        print('\\nEpoch: %d' % epoch)\n","        net.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","        idx = 0\n","        for batch_idx, (inputs, targets) in enumerate(trainloader):\n","            idx = batch_idx\n","            if inputs.shape[0] < batch_size:\n","                continue\n","            if use_cuda:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","            inputs, targets = Variable(inputs), Variable(targets)\n","\n","            # update learning rate\n","            for nlr in range(len(optimizer.param_groups)):\n","                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, nb_epoch, lr[nlr])\n","\n","            with torch.no_grad():\n","                _, _, _, output_teacher = net_teacher(inputs)\n","\n","            optimizer.zero_grad()\n","            output = net(inputs)\n","            loss = 0.7*criterionKD(output, output_teacher) + 0.3 * CELoss(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            #  training log\n","            _, predicted = torch.max(output.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","\n","            train_loss += loss.item()\n","\n","            if batch_idx % 50 == 0 or batch_idx == trainloader.__len__() - 1:\n","                print(\n","                    'Step: %d | Loss1: %.3f | Acc: %.3f%% (%d/%d)' % (\n","                        batch_idx, train_loss / (batch_idx + 1),\n","                        100. * float(correct) / total, correct, total))\n","\n","        train_acc = 100. * float(correct) / total\n","        train_loss = train_loss / (idx + 1)\n","        with open(exp_dir + '/results_train.txt', 'a') as file:\n","            file.write(\n","                'Iteration %d | train_acc = %.5f | train_loss = %.5f |\\n' % (\n","                    epoch, train_acc, train_loss))\n","\n","        if epoch < 50 :\n","            val_acc, val_loss = test_search(net, CELoss, 16)\n","            if val_acc > max_val_acc:\n","                max_val_acc = val_acc\n","                net.cpu()\n","                torch.save(net, store_name + '/model%.5f.pth'%(max_val_acc))\n","                net.to(device)\n","            with open(exp_dir + '/results_test.txt', 'a') as file:\n","                file.write('IteratiFon %d, test_acc = %.5f, test_loss = %.6f\\n' % (\n","                    epoch, val_acc, val_loss))\n","        else:\n","            net.cpu()\n","            torch.save(net,  store_name + '/model%.5f.pth'%(max_val_acc))\n","            net.to(device)\n","\n","\n","if __name__ == '__main__':\n","    save_path = '/content/gdrive/MyDrive/STF/save_search'\n","    if not os.path.exists(save_path):\n","        os.mkdir(save_path)\n","    train(nb_epoch=300,  # number of epoch\n","          batch_size=32, # batch size\n","          store_name=save_path,  # folder for output\n","          resume=True,  # resume training from checkpoint\n","          start_epoch=0,  # the start epoch number when you resume the training\n","          model_path='/content/gdrive/MyDrive/STF/save_search/model71.pth')  # the saved model where you want to resume the training"]},{"cell_type":"markdown","metadata":{"id":"p8EkbX5E66zq"},"source":["Train driver transfer"]},{"cell_type":"markdown","metadata":{"id":"tpYSLCDa8Yu4"},"source":["Utils distill"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcenLymC8X-6"},"outputs":[],"source":["import numpy as np\n","import torchvision\n","from torch.autograd import Variable\n","from torchvision import transforms\n","#from model import *\n","\n","\n","def cosine_anneal_schedule(t, nb_epoch, lr):\n","    cos_inner = np.pi * (t % (nb_epoch))  # t - 1 is used when t has 1-based indexing.\n","    cos_inner /= (nb_epoch)\n","    cos_out = np.cos(cos_inner) + 1\n","\n","    return float(lr / 2 * cos_out)\n","\n","\n","def model_info(model):  # Plots a line-by-line description of a PyTorch model\n","    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n","    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n","    print('\\n%5s %50s %9s %12s %20s %12s %12s' % ('layer', 'name', 'gradient', 'parameters', 'shape', 'mu', 'sigma'))\n","    for i, (name, p) in enumerate(model.named_parameters()):\n","        name = name.replace('module_list.', '')\n","        print('%5g %50s %9s %12g %20s %12.3g %12.3g' % (\n","            i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n","    print('Model Summary: %g layers, %g parameters, %g gradients\\n' % (i + 1, n_p, n_g))\n","\n","\n","def test(net, net_teacher, criterion, batch_size):\n","    net.eval()\n","    net_teacher.eval()\n","    use_cuda = torch.cuda.is_available()\n","    test_loss = 0\n","    correct = 0\n","    correct_t = 0\n","    correct_com = 0\n","    total = 0\n","    idx = 0\n","    device = torch.device(\"cuda\")\n","\n","    transform_test = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    testset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/test1',\n","                                               transform=transform_test)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            idx = batch_idx\n","            if use_cuda:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","            inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n","            output = net(inputs)\n","            _, _, _, output_t = net_teacher(inputs)\n","\n","            loss = criterion(output, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = torch.max(output.data, 1)\n","            _, predicted_t = torch.max(output_t.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","            correct_t += predicted_t.eq(targets.data).cpu().sum()\n","\n","            if batch_idx % 50 == 0 or batch_idx == testloader.__len__()-1:\n","                print('Step: %d | Loss: %.3f | Acc: %.3f%% (%d/%d) |' % (\n","                batch_idx, test_loss / (batch_idx + 1), 100. * float(correct) / total, correct, total))\n","\n","    test_acc = 100. * float(correct) / total\n","\n","    test_loss = test_loss / (idx + 1)\n","\n","\n","    return test_acc, test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jD7c-i0WI7Z5"},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import print_function\n","from __future__ import division\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Logits(nn.Module):\n","\tdef __init__(self):\n","\t\tsuper(Logits, self).__init__()\n","\n","\tdef forward(self, out_s, out_t):\n","\t\tloss = F.mse_loss(out_s, out_t)\n","\n","\t\treturn loss\n"]},{"cell_type":"markdown","metadata":{"id":"_XsQbU7svlEd"},"source":["sAVE TRANFER  "]},{"cell_type":"markdown","metadata":{"id":"t__5GfQCyhgn"},"source":["dummy   changed architecture which reduced parameter size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0uZpzIjg9cxj"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","# changed architecture which reduced parameter size\n","class BasicLayer(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, kernel_size_, group=1, group_=4):\n","        super(BasicLayer, self).__init__()\n","        self.convs = nn.Sequential(\n","            PyConv2(in_planes, out_planes, pyconv_kernels=[kernel_size, kernel_size_], stride=1, pyconv_groups=[group, group_]),\n","            nn.BatchNorm2d(out_planes),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.pool = nn.MaxPool2d(2, stride=2)\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        x = self.pool(x)\n","        return x\n","\n","class Searched_Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # changed architecture which reduced parameter size\n","        self.convs = nn.Sequential(\n","            BasicLayer(3, 16, kernel_size=3, kernel_size_=3, group=1, group_=1),  # Reduced channels to 16\n","            BasicLayer(16, 32, kernel_size=3, kernel_size_=3, group=1, group_=1),  # Reduced channels to 32\n","            BasicLayer(32, 64, kernel_size=3,  kernel_size_=3, group=1, group_=1),  # Reduced channels to 64\n","            BasicLayer(64, 128, kernel_size=3,  kernel_size_=3, group=1, group_=1)  # Reduced channels to 128\n","        )\n","        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(128, 10)  # Reduced input size to 128\n","        )\n","\n","    def forward(self, x):\n","        x = self.convs(x)\n","        x = self.pool(x).view(x.shape[0], -1)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfEZpnQa9Ae5"},"outputs":[],"source":["def train(nb_epoch, batch_size, store_name, resume=False, start_epoch=0, model_path=None):\n","    # setup output\n","\n","    #if args.kd_mode == 'logits':\n","    kd_mode = 'logits'\n","    criterionKD = Logits()\n","\n","\n","    exp_dir = store_name\n","    try:\n","        os.stat(exp_dir)\n","    except:\n","        os.makedirs(exp_dir)\n","\n","    use_cuda = True\n","    print(use_cuda)\n","\n","\n","    # Data\n","    print('==> Preparing data..')\n","    transform_train = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.RandomCrop(224, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    trainset = torchvision.datasets.ImageFolder(root=\"/content/gdrive/MyDrive/STF/train\",\n","                                                transform=transform_train)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    net_teacher = torch.load(\"/content/gdrive/MyDrive/STF/save_teacher/model99.510.pth\")\n","    net = Searched_Net()\n","    model_info(net)\n","    print('Model %s created, param count: %d' %\n","          ('Created_model', sum([m.numel() for m in net.parameters()])))\n","\n","\n","    netp_teacher = torch.nn.DataParallel(net_teacher).cuda()\n","    netp = torch.nn.DataParallel(net).cuda()\n","    device = torch.device(\"cuda\")\n","    cudnn.benchmark = True\n","\n","    CELoss = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9, weight_decay=5e-4)\n","\n","    max_val_acc = 0\n","    lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n","    for epoch in range(start_epoch, nb_epoch):\n","        print('\\nEpoch: %d' % epoch)\n","        net.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","        idx = 0\n","        for batch_idx, (inputs, targets) in enumerate(trainloader):\n","            idx = batch_idx\n","            if inputs.shape[0] < batch_size:\n","                continue\n","            if use_cuda:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","            inputs, targets = Variable(inputs), Variable(targets)\n","\n","            # update learning rate\n","            for nlr in range(len(optimizer.param_groups)):\n","                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, nb_epoch, lr[nlr])\n","\n","            if kd_mode in ['sobolev', 'lwm']:\n","                inputs.requires_grad = True\n","\n","            with torch.no_grad():\n","                _, _, _, output_teacher = netp_teacher(inputs)\n","\n","\n","            optimizer.zero_grad()\n","            output = netp(inputs)\n","\n","            if kd_mode in ['sobolev']:\n","                loss = criterionKD(output, output_teacher, inputs, targets)\n","            else:\n","                loss = 0.7*criterionKD(output, output_teacher) + 0.3 * CELoss(output, targets)\n","\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            #  training log\n","            _, predicted = torch.max(output.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","\n","            train_loss += loss.item()\n","\n","            if batch_idx % 50 == 0 or batch_idx == trainloader.__len__() - 1:\n","                print(\n","                    'Step: %d | Loss1: %.3f | Acc: %.3f%% (%d/%d)' % (\n","                        batch_idx, train_loss / (batch_idx + 1),\n","                        100. * float(correct) / total, correct, total))\n","\n","        train_acc = 100. * float(correct) / total\n","        train_loss = train_loss / (idx + 1)\n","        with open(exp_dir + '/results_train.txt', 'a') as file:\n","            file.write(\n","                'Iteration %d | train_acc = %.5f | train_loss = %.5f |\\n' % (\n","                    epoch, train_acc, train_loss))\n","        val_acc, val_loss = test(net, netp_teacher, CELoss, 16)\n","        if  epoch <= 500:\n","\n","            if val_acc > max_val_acc:\n","                max_val_acc = val_acc\n","                net.cpu()\n","                torch.save(net,  store_name + '/model %.5f.pth'%(max_val_acc))\n","                net.to(device)\n","            with open(exp_dir + '/results_test.txt', 'a') as file:\n","                file.write('Iteration %d, test_acc = %.5f, test_loss = %.6f\\n' % (\n","                    epoch, val_acc, val_loss))\n","        else:\n","            net.cpu()\n","            torch.save(net, store_name + '/model %.5f.pth'%(val_acc))\n","            net.to(device)\n","\n","if __name__ == '__main__':\n","    save_path = '/content/gdrive/MyDrive/STF/dummy_save_transfer_' + 'ST'\n","    if not os.path.exists(save_path):\n","        os.mkdir(save_path)\n","    train(nb_epoch=300,  # number of epoch\n","          batch_size=32,  # batch size\n","          store_name=save_path,  # folder for output\n","          resume=True,  # resume training from checkpoint\n","          start_epoch=4,  # the start epoch number when you resume the training\n","          model_path='/content/gdrive/MyDrive/STF/dummy_save_transfer_ST/model 40.47151.pth')  # the saved model where you want to resume the training"]},{"cell_type":"markdown","metadata":{"id":"IBLtLgMK--mJ"},"source":["finetunetest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQaI2EH5_Arn"},"outputs":[],"source":["def finetunetest(net,  criterion, batch_size):\n","    net.eval()\n","\n","    use_cuda = torch.cuda.is_available()\n","    test_loss = 0\n","    correct = 0\n","    correct_t = 0\n","    correct_com = 0\n","    total = 0\n","    idx = 0\n","    device = torch.device(\"cuda\")\n","\n","\n","    transform_test = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.RandomCrop(224, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    testset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/test1',\n","                                               transform=transform_test)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            idx = batch_idx\n","            if use_cuda:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","            inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n","            output = net(inputs)\n","           # _, _, _, output_t = net(inputs)\n","\n","\n","            loss = criterion(output, targets)\n","\n","\n","            test_loss += loss.item()\n","            _, predicted = torch.max(output.data, 1)\n","           # , predicted_t = torch.max(output_t.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","          #  correct_t += predicted_t.eq(targets.data).cpu().sum()\n","\n","\n","            if batch_idx % 50 == 0 or batch_idx == testloader.__len__()-1:\n","                print('Step: %d | Loss: %.3f | Acc: %.3f%% (%d/%d) |' % (\n","                batch_idx, test_loss / (batch_idx + 1), 100. * float(correct) / total, correct, total))\n","\n","\n","    test_acc = 100. * float(correct) / total\n","\n","\n","    test_loss = test_loss / (idx + 1)\n","\n","\n","\n","    return test_acc, test_loss"]},{"cell_type":"markdown","metadata":{"id":"MHpbPzScW_2W"},"source":["Fintune\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzYidGIbY571"},"outputs":[],"source":["\n","def train(nb_epoch, batch_size, store_name, resume=False, start_epoch=0, model_path=None):\n","    _logger = logging.getLogger('train')\n","    # setup output\n","    exp_dir = store_name\n","    try:\n","        os.stat(exp_dir)\n","    except:\n","        os.makedirs(exp_dir)\n","\n","    use_cuda = torch.cuda.is_available()\n","    print(use_cuda)\n","\n","    print('==> Preparing data..')\n","    transform_train = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.RandomCrop(224, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ])\n","    trainset = torchvision.datasets.ImageFolder(root='/content/gdrive/MyDrive/STF/train',\n","                                                transform=transform_train)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","    #if args.OE:\n","    #    net = torch.load(\"save_transfer_OE_logits/model.pth\")\n","    #else:\n","    net = torch.load(\"/content/gdrive/MyDrive/STF/dummy_save_transfer_ST/model 99.21415.pth\")\n","\n","    print('Model %s created, param count: %d' %\n","          ('Created_model', sum([m.numel() for m in net.parameters()])))\n","\n","\n","    netp = torch.nn.DataParallel(net).cuda()\n","    #nets = torch.nn.DataParallel(netp).cuda()\n","\n","    device = torch.device(\"cuda\")\n","    net.to(device)\n","\n","\n","    CELoss = nn.CrossEntropyLoss()\n","\n","    optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9, weight_decay=5e-4)\n","\n","    max_val_acc = 0\n","    lr = [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n","    for epoch in range(start_epoch, nb_epoch):\n","        print('\\nEpoch: %d' % epoch)\n","        net.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","        idx = 0\n","        for batch_idx, (inputs, targets) in enumerate(trainloader):\n","            idx = batch_idx\n","            if inputs.shape[0] < batch_size:\n","                continue\n","            if use_cuda:\n","                inputs, targets = inputs.to(device), targets.to(device)\n","            inputs, targets = Variable(inputs), Variable(targets)\n","\n","            for nlr in range(len(optimizer.param_groups)):\n","                optimizer.param_groups[nlr]['lr'] = cosine_anneal_schedule(epoch, nb_epoch, lr[nlr])\n","\n","            optimizer.zero_grad()\n","            output = netp(inputs)\n","            loss = CELoss(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            #  training log\n","            _, predicted = torch.max(output.data, 1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets.data).cpu().sum()\n","\n","            train_loss += loss.item()\n","\n","            if batch_idx % 50 == 0 or batch_idx == trainloader.__len__() - 1:\n","                print(\n","                    'Step: %d | Loss1: %.3f | Acc: %.3f%% (%d/%d)' % (\n","                        batch_idx, train_loss / (batch_idx + 1),\n","                        100. * float(correct) / total, correct, total))\n","\n","        train_acc = 100. * float(correct) / total\n","        train_loss = train_loss / (idx + 1)\n","        with open(exp_dir + '/results_train.txt', 'a') as file:\n","            file.write(\n","                'Iteration %d | train_acc = %.5f | train_loss = %.5f |\\n' % (\n","                    epoch, train_acc, train_loss))\n","        val_acc, val_loss = finetunetest(netp, CELoss, 16)\n","\n","            #val_acc, val_loss = finetunetest(netp, CELoss, 16)\n","        if val_acc > max_val_acc:\n","            max_val_acc = val_acc\n","            net.cpu()\n","            torch.save(net, store_name + '/model %.5f.pth'%(max_val_acc))\n","            net.to(device)\n","        with open(exp_dir + '/results_test.txt', 'a') as file:\n","            file.write('Iteration %d, test_acc = %.5f, test_loss = %.6f\\n' % (\n","                epoch, val_acc, val_loss))\n","\n","\n","\n","if __name__ == '__main__':\n","\n","    save_path = \"/content/gdrive/MyDrive/STF/dummy_save_finetune\"\n","\n","    if not os.path.exists(save_path):\n","        os.mkdir(save_path)\n","    train(nb_epoch=500,  # number of epoch\n","          batch_size=32,  # batch size\n","          store_name=save_path,  # folder for output\n","          resume=False,  # resume training from checkpoint\n","          start_epoch=0,  # the start epoch number when you resume the training\n","          model_path='')  # the saved model where you want to resume the training"]},{"cell_type":"markdown","metadata":{"id":"97TqVzkDZqQY"},"source":["Prediction"]},{"cell_type":"code","source":["from __future__ import division\n","import torch\n","torch.manual_seed(0)\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","from PIL import Image, ImageOps, ImageEnhance\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomCrop(224, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","model=torch.load('/content/gdrive/MyDrive/STF/dummy_save_finetune/model 99.98.pth')  # Load pretrained parameters\n","model.eval()  # Set to eval mode to change behavior of Dropout, BatchNorm\n","\n","predictions=[\"safe driving\",\"texting - right\",\"talking on the phone - right\",\"texting - left\",\"talking on the phone - left\",\"operating the radio\",\"drinking\",\n","\"reaching behind\",\"hair and makeup\", \"talking to passenger\"]\n","\n","path=\"/content/gdrive/MyDrive/STF/test1/c8/img_100015.jpg\"\n","\n","img = Image.open(path)  # Load image as PIL.Image\n","image=cv2.imread(path)\n","x = transform(img)  # Preprocess image\n","x = x.unsqueeze(0)  # Add batch dimension\n","\n","output = model(x)  # Forward pass\n","pred = torch.argmax(output, 1)  # Get predicted class if multi-class classification\n","print('Image predicted as ', predictions[pred[0]])\n","window=predictions[pred[0]]\n","cv2_imshow(image)"],"metadata":{"id":"Qrs3xNZKXfGT"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Pci-PnL7fivJhXKqgUy2hGYdTWuJVSe_","timestamp":1706119125846}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}